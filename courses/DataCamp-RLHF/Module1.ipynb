{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c7f103",
   "metadata": {},
   "source": [
    "### Text-generation with pre-trained RLHF Hugging Face models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c9dce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from scikit-learn) (2.4.1)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.8.0-cp313-cp313-win_amd64.whl (8.0 MB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp313-cp313-win_amd64.whl (36.3 MB)\n",
      "   ---------------------------------------- 0.0/36.3 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 11.0/36.3 MB 54.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 21.8/36.3 MB 54.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 25.7/36.3 MB 41.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 34.6/36.3 MB 41.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 36.3/36.3 MB 39.1 MB/s  0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0c9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16802d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cheku\\GenAI-RoadMap\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cheku\\.cache\\huggingface\\hub\\models--lvwerra--gpt2-imdb-pos-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated review: [{'generated_text': 'It felt like a great start for the movie, but I was very impressed by how much music and artwork we saw. We had a lot of fun with the whole movie. It was a very fun movie to see it. It was a very fun movie to see the movie. It was a fun movie to see the movie because of many of the people who have recorded it. It was a very fun movie to see the movie because of many people who have the freedom to have it. It was a very fun movie to see all the people who have made it, and it was a lot fun to see the movie with the music and the music. It was a very fun movie to see the movie with the story and the movie. It was a very fun movie to see the movie with the music and the movies. It was a very fun movie to see the movie with the movie and the movie. It was a very fun movie to see the movie to see the movie because of many of the people who have made it. It was a very fun movie to see the movie because of many of the people who have made it. It was a very fun movie to see the movie because of many of the people who have made it. It was a very fun movie to see the movie because of many people who have all the'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lvwerra/gpt2-imdb-pos-v2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_generator = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\n",
    "review_prompt = \"It felt like a great start for the movie, but\"\n",
    "generated_review = text_generator(review_prompt, max_length=20, truncation=True)\n",
    "print(\"Generated review:\",generated_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f6f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=8) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The start was a great one, but the end disappointed me. was very good. The story was very'}]\n"
     ]
    }
   ],
   "source": [
    "generated_review = text_generator(\"The start was a great one, but the end disappointed me.\", max_new_tokens=8, max_length=10, truncation=True)\n",
    "print(generated_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24385ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=3) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Suprisingly, the film is a great masterpiece.'}]\n"
     ]
    }
   ],
   "source": [
    "generated_review = text_generator(\"Suprisingly, the film is a\", max_new_tokens=3, max_length=10, truncation=True)\n",
    "print(generated_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7584eb",
   "metadata": {},
   "source": [
    "### Classifying the sentiment of the generated review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0fb523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "sentiment_analyzer = pipeline('sentiment-analysis',model=DistilBertForSequenceClassification.from_pretrained('lvwerra/distilbert-imdb'),tokenizer=DistilBertTokenizerFast.from_pretrained('lvwerra/distilbert-imdb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589dbd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9941137433052063}]\n"
     ]
    }
   ],
   "source": [
    "review_text = \"Suprisingly, the film is a very good one\" # One of the generations\n",
    "sentiment = sentiment_analyzer(review_text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce917a",
   "metadata": {},
   "source": [
    "### Fine-tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502bb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20491/20491 [00:03<00:00, 6455.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"argilla/tripadvisor-hotel-reviews\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'],padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9483329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DatasetDict in module datasets.dataset_dict object:\n",
      "\n",
      "class DatasetDict(builtins.dict)\n",
      " |  A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      DatasetDict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __enter__(self)\n",
      " |\n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |\n",
      " |  __getitem__(self, k) -> datasets.arrow_dataset.Dataset\n",
      " |      Return self[key].\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  align_labels_with_mapping(self, label2id: dict, label_column: str) -> 'DatasetDict'\n",
      " |      Align the dataset's label ID and label name mapping to match an input `label2id` mapping.\n",
      " |      This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n",
      " |      The alignment in done using the lowercase label names.\n",
      " |\n",
      " |      Args:\n",
      " |          label2id (`dict`):\n",
      " |              The label name to ID mapping to align the dataset with.\n",
      " |          label_column (`str`):\n",
      " |              The column name of labels to align on.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
      " |      >>> ds = load_dataset(\"nyu-mll/glue\", \"mnli\", split=\"train\")\n",
      " |      >>> # mapping to align with\n",
      " |      >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
      " |      >>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
      " |      ```\n",
      " |\n",
      " |  cast(self, features: datasets.features.features.Features) -> 'DatasetDict'\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          features ([`Features`]):\n",
      " |              New features to cast the dataset to.\n",
      " |              The name and order of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. `string` <-> `ClassLabel` you should use [`~DatasetDict.map`] to update the dataset.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset, ClassLabel, Value\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(names=['neg', 'pos']),\n",
      " |       'text': Value('string')}\n",
      " |      >>> new_features = ds[\"train\"].features.copy()\n",
      " |      >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      " |      >>> new_features['text'] = Value('large_string')\n",
      " |      >>> ds = ds.cast(new_features)\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(names=['bad', 'good']),\n",
      " |       'text': Value('large_string')}\n",
      " |      ```\n",
      " |\n",
      " |  cast_column(self, column: str, feature) -> 'DatasetDict'\n",
      " |      Cast column to feature for decoding.\n",
      " |\n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              Column name.\n",
      " |          feature ([`Feature`]):\n",
      " |              Target feature.\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset, ClassLabel\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(names=['neg', 'pos']),\n",
      " |       'text': Value('string')}\n",
      " |      >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(names=['bad', 'good']),\n",
      " |       'text': Value('string')}\n",
      " |      ```\n",
      " |\n",
      " |  class_encode_column(self, column: str, include_nulls: bool = False) -> 'DatasetDict'\n",
      " |      Casts the given column as [`~datasets.features.ClassLabel`] and updates the tables.\n",
      " |\n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              The name of the column to cast.\n",
      " |          include_nulls (`bool`, defaults to `False`):\n",
      " |              Whether to include null values in the class labels. If `True`, the null values will be encoded as the `\"None\"` class label.\n",
      " |\n",
      " |              <Added version=\"1.14.2\"/>\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"google/boolq\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'answer': Value('bool'),\n",
      " |       'passage': Value('string'),\n",
      " |       'question': Value('string')}\n",
      " |      >>> ds = ds.class_encode_column(\"answer\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'answer': ClassLabel(num_classes=2, names=['False', 'True']),\n",
      " |       'passage': Value('string'),\n",
      " |       'question': Value('string')}\n",
      " |      ```\n",
      " |\n",
      " |  cleanup_cache_files(self) -> dict[str, int]\n",
      " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n",
      " |      Be careful when running this command that no other process is currently using other cache files.\n",
      " |\n",
      " |      Return:\n",
      " |          `Dict` with the number of removed files for each split\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.cleanup_cache_files()\n",
      " |      {'test': 0, 'train': 0, 'validation': 0}\n",
      " |      ```\n",
      " |\n",
      " |  filter(\n",
      " |      self,\n",
      " |      function: Optional[Callable] = None,\n",
      " |      with_indices: bool = False,\n",
      " |      with_rank: bool = False,\n",
      " |      input_columns: Union[str, list[str], NoneType] = None,\n",
      " |      batched: bool = False,\n",
      " |      batch_size: Optional[int] = 1000,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      load_from_cache_file: Optional[bool] = None,\n",
      " |      cache_file_names: Optional[dict[str, Optional[str]]] = None,\n",
      " |      writer_batch_size: Optional[int] = 1000,\n",
      " |      fn_kwargs: Optional[dict] = None,\n",
      " |      num_proc: Optional[int] = None,\n",
      " |      desc: Optional[str] = None\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          function (`Callable`): Callable with one of the following signatures:\n",
      " |\n",
      " |              - `function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |              - `function(batch: Dict[str, list]) -> list[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(batch: Dict[str, list], *extra_args) -> list[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |\n",
      " |              If no function is provided, defaults to an always `True` function: `lambda x: True`.\n",
      " |          with_indices (`bool`, defaults to `False`):\n",
      " |              Provide example indices to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
      " |          with_rank (`bool`, defaults to `False`):\n",
      " |              Provide process rank to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      " |          input_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):\n",
      " |              The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`):\n",
      " |              Provide batch of examples to `function`.\n",
      " |          batch_size (`int`, *optional*, defaults to `1000`):\n",
      " |              Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          fn_kwargs (`Dict`, *optional*, defaults to `None`):\n",
      " |              Keyword arguments to be passed to `function`\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |               The number of processes to use for multiprocessing.\n",
      " |              - If `None` or `0`, no multiprocessing is used and the operation runs in the main process.\n",
      " |              - If greater than `1`, one or multiple worker processes are used to process data in parallel.\n",
      " |               Note: The function passed to `map()` must be picklable for multiprocessing to work correctly\n",
      " |               (i.e., prefer functions defined at the top level of a module, not inside another function or class).\n",
      " |          desc (`str`, *optional*, defaults to `None`):\n",
      " |              Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 4265\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 533\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 533\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  flatten(self, max_depth=16) -> 'DatasetDict'\n",
      " |      Flatten the Apache Arrow Table of each split (nested features are flatten).\n",
      " |      Each column with a struct type is flattened into one column per struct field.\n",
      " |      Other columns are left unchanged.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rajpurkar/squad\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'id': Value('string'),\n",
      " |       'title': Value('string'),\n",
      " |       'context': Value('string'),\n",
      " |       'question': Value('string'),\n",
      " |       'answers.text': List(Value('string')),\n",
      " |       'answers.answer_start': List(Value('int32'))}\n",
      " |      >>> ds.flatten()\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " |              num_rows: 87599\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " |              num_rows: 10570\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  flatten_indices(\n",
      " |      self,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      cache_file_names: Optional[dict[str, Optional[str]]] = None,\n",
      " |      writer_batch_size: Optional[int] = 1000,\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      disable_nullable: bool = False,\n",
      " |      num_proc: Optional[int] = None,\n",
      " |      new_fingerprint: Optional[str] = None\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create and cache a new Dataset by flattening the indices mapping.\n",
      " |\n",
      " |      Args:\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          cache_file_names (`Dict[str, str]`, *optional*, default `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          features (`Optional[datasets.Features]`, defaults to `None`):\n",
      " |              Use a specific [`Features`] to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `False`):\n",
      " |              Allow null values in the table.\n",
      " |          num_proc (`int`, optional, default `None`):\n",
      " |              Max number of processes when generating cache. Already cached shards are loaded sequentially\n",
      " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
      " |              The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |\n",
      " |  formatted_as(\n",
      " |      self,\n",
      " |      type: Optional[str] = None,\n",
      " |      columns: Optional[list] = None,\n",
      " |      output_all_columns: bool = False,\n",
      " |      **format_kwargs\n",
      " |  )\n",
      " |      To be used in a `with` statement. Set `__getitem__` return format (type and columns).\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          type (`str`, *optional*):\n",
      " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'jax', 'arrow', 'pandas', 'polars']`.\n",
      " |              `None` means `__getitem__` returns python objects (default).\n",
      " |          columns (`list[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              `None` means `__getitem__` returns all columns (default).\n",
      " |          output_all_columns (`bool`, defaults to False):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |          **format_kwargs (additional keyword arguments):\n",
      " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |\n",
      " |  map(\n",
      " |      self,\n",
      " |      function: Optional[Callable] = None,\n",
      " |      with_indices: bool = False,\n",
      " |      with_rank: bool = False,\n",
      " |      with_split: bool = False,\n",
      " |      input_columns: Union[str, list[str], NoneType] = None,\n",
      " |      batched: bool = False,\n",
      " |      batch_size: Optional[int] = 1000,\n",
      " |      drop_last_batch: bool = False,\n",
      " |      remove_columns: Union[str, list[str], NoneType] = None,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      load_from_cache_file: Optional[bool] = None,\n",
      " |      cache_file_names: Optional[dict[str, Optional[str]]] = None,\n",
      " |      writer_batch_size: Optional[int] = 1000,\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      disable_nullable: bool = False,\n",
      " |      fn_kwargs: Optional[dict] = None,\n",
      " |      num_proc: Optional[int] = None,\n",
      " |      desc: Optional[str] = None,\n",
      " |      try_original_type: Optional[bool] = True\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Apply a function to all the examples in the table (individually or in batches) and update the table.\n",
      " |      If your function returns a column that already exists, then it overwrites it.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      You can specify whether the function should be batched or not with the `batched` parameter:\n",
      " |\n",
      " |      - If batched is `False`, then the function takes 1 example in and should return 1 example.\n",
      " |        An example is a dictionary, e.g. `{\"text\": \"Hello there !\"}`.\n",
      " |      - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n",
      " |        A batch is a dictionary, e.g. a batch of 1 example is `{\"text\": [\"Hello there !\"]}`.\n",
      " |      - If batched is `True` and `batch_size` is `n > 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\n",
      " |        Note that the last batch may have less than `n` examples.\n",
      " |        A batch is a dictionary, e.g. a batch of `n` examples is `{\"text\": [\"Hello there !\"] * n}`.\n",
      " |\n",
      " |      If the function is asynchronous, then `map` will run your function in parallel, with up to one thousand simultaneous calls.\n",
      " |      It is recommended to use a `asyncio.Semaphore` in your function if you want to set a maximum number of operations that can run at the same time.\n",
      " |\n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n",
      " |              - `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n",
      " |              - `function(batch: Dict[str, list]) -> Dict[str, list]` if `batched=True` and `with_indices=False`\n",
      " |              - `function(batch: Dict[str, list], indices: list[int]) -> Dict[str, list]` if `batched=True` and `with_indices=True`\n",
      " |\n",
      " |              For advanced usage, the function can also return a `pyarrow.Table`.\n",
      " |              If the function is asynchronous, then `map` will run your function in parallel.\n",
      " |              Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
      " |              If no function is provided, default to identity function: `lambda x: x`.\n",
      " |          with_indices (`bool`, defaults to `False`):\n",
      " |              Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          with_rank (`bool`, defaults to `False`):\n",
      " |              Provide process rank to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      " |          with_split (`bool`, defaults to `False`):\n",
      " |              Provide process split to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], split): ...`.\n",
      " |          input_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):\n",
      " |              The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`):\n",
      " |              Provide batch of examples to `function`.\n",
      " |          batch_size (`int`, *optional*, defaults to `1000`):\n",
      " |              Number of examples per batch provided to `function` if `batched=True`,\n",
      " |              `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n",
      " |          drop_last_batch (`bool`, defaults to `False`):\n",
      " |              Whether a last batch smaller than the batch_size should be\n",
      " |              dropped instead of being processed by the function.\n",
      " |          remove_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):\n",
      " |              Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, default `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |          features (`[datasets.Features]`, *optional*, defaults to `None`):\n",
      " |              Use a specific [`Features`] to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `False`):\n",
      " |              Disallow null values in the table.\n",
      " |          fn_kwargs (`Dict`, *optional*, defaults to `None`):\n",
      " |              Keyword arguments to be passed to `function`\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |               The number of processes to use for multiprocessing.\n",
      " |              - If `None` or `0`, no multiprocessing is used and the operation runs in the main process.\n",
      " |              - If greater than `1`, one or multiple worker processes are used to process data in parallel.\n",
      " |               Note: The function passed to `map()` must be picklable for multiprocessing to work correctly\n",
      " |               (i.e., prefer functions defined at the top level of a module, not inside another function or class).\n",
      " |          desc (`str`, *optional*, defaults to `None`):\n",
      " |              Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      " |          try_original_type (`Optional[bool]`, defaults to `True`):\n",
      " |              Try to keep the types of the original columns (e.g. int32 -> int32).\n",
      " |              Set to False if you want to always infer new types.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> def add_prefix(example):\n",
      " |      ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      " |      ...     return example\n",
      " |      >>> ds = ds.map(add_prefix)\n",
      " |      >>> ds[\"train\"][0:3][\"text\"]\n",
      " |      ['Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
      " |       'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',\n",
      " |       'Review: effective but too-tepid biopic']\n",
      " |\n",
      " |      # process a batch of examples\n",
      " |      >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      " |      # set number of processors\n",
      " |      >>> ds = ds.map(add_prefix, num_proc=4)\n",
      " |      ```\n",
      " |\n",
      " |  push_to_hub(\n",
      " |      self,\n",
      " |      repo_id,\n",
      " |      config_name: str = 'default',\n",
      " |      set_default: Optional[bool] = None,\n",
      " |      data_dir: Optional[str] = None,\n",
      " |      commit_message: Optional[str] = None,\n",
      " |      commit_description: Optional[str] = None,\n",
      " |      private: Optional[bool] = None,\n",
      " |      token: Optional[str] = None,\n",
      " |      revision: Optional[str] = None,\n",
      " |      create_pr: Optional[bool] = False,\n",
      " |      max_shard_size: Union[str, int, NoneType] = None,\n",
      " |      num_shards: Optional[dict[str, int]] = None,\n",
      " |      embed_external_files: bool = True,\n",
      " |      num_proc: Optional[int] = None\n",
      " |  ) -> huggingface_hub.hf_api.CommitInfo\n",
      " |      Pushes the [`DatasetDict`] to the hub as a Parquet dataset.\n",
      " |      The [`DatasetDict`] is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      " |\n",
      " |      Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n",
      " |\n",
      " |      The resulting Parquet files are self-contained by default: if your dataset contains [`Image`] or [`Audio`]\n",
      " |      data, the Parquet files will store the bytes of your images or audio files.\n",
      " |      You can disable this by setting `embed_external_files` to False.\n",
      " |\n",
      " |      Args:\n",
      " |          repo_id (`str`):\n",
      " |              The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n",
      " |              `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n",
      " |              of the logged-in user.\n",
      " |          config_name (`str`):\n",
      " |              Configuration name of a dataset. Defaults to \"default\".\n",
      " |          set_default (`bool`, *optional*):\n",
      " |              Whether to set this configuration as the default one. Otherwise, the default configuration is the one\n",
      " |              named \"default\".\n",
      " |          data_dir (`str`, *optional*):\n",
      " |              Directory name that will contain the uploaded data files. Defaults to the `config_name` if different\n",
      " |              from \"default\", else \"data\".\n",
      " |\n",
      " |              <Added version=\"2.17.0\"/>\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload dataset\"`.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              Description of the commit that will be created.\n",
      " |              Additionally, description of the PR if a PR is created (`create_pr` is True).\n",
      " |\n",
      " |              <Added version=\"2.16.0\"/>\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the\n",
      " |              organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`str`, *optional*):\n",
      " |              An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
      " |              to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n",
      " |              if no token is passed and the user is not logged-in.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to. Defaults to the `\"main\"` branch.\n",
      " |\n",
      " |              <Added version=\"2.15.0\"/>\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to create a PR with the uploaded files or directly commit.\n",
      " |\n",
      " |              <Added version=\"2.15.0\"/>\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
      " |              (like `\"500MB\"` or `\"1GB\"`).\n",
      " |          num_shards (`Dict[str, int]`, *optional*):\n",
      " |              Number of shards to write. By default, the number of shards depends on `max_shard_size`.\n",
      " |              Use a dictionary to define a different num_shards for each split.\n",
      " |\n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          embed_external_files (`bool`, defaults to `True`):\n",
      " |              Whether to embed file bytes in the shards.\n",
      " |              In particular, this will do the following before the push for the fields of type:\n",
      " |\n",
      " |              - [`Audio`] and [`Image`] removes local path information and embed file content in the Parquet files.\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes when preparing and uploading the dataset.\n",
      " |              This is helpful if the dataset is made of many samples or media files to embed.\n",
      " |              Multiprocessing is disabled by default.\n",
      " |\n",
      " |              <Added version=\"4.0.0\"/>\n",
      " |\n",
      " |      Return:\n",
      " |          huggingface_hub.CommitInfo\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n",
      " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n",
      " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n",
      " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", num_shards={\"train\": 1024, \"test\": 8})\n",
      " |      ```\n",
      " |\n",
      " |      If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n",
      " |\n",
      " |      ```python\n",
      " |      >>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n",
      " |      >>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n",
      " |      >>> # later\n",
      " |      >>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n",
      " |      >>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n",
      " |      ```\n",
      " |\n",
      " |  remove_columns(self, column_names: Union[str, list[str]]) -> 'DatasetDict'\n",
      " |      Remove one or several column(s) from each split in the dataset\n",
      " |      and the features associated to the column(s).\n",
      " |\n",
      " |      The transformation is applied to all the splits of the dataset dictionary.\n",
      " |\n",
      " |      You can also remove a column using [`~DatasetDict.map`] with `remove_columns` but the present method\n",
      " |      doesn't copy the data of the remaining columns and is thus faster.\n",
      " |\n",
      " |      Args:\n",
      " |          column_names (`Union[str, list[str]]`):\n",
      " |              Name of the column(s) to remove.\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]: A copy of the dataset object without the columns to remove.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds = ds.remove_columns(\"label\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  rename_column(self, original_column_name: str, new_column_name: str) -> 'DatasetDict'\n",
      " |      Rename a column in the dataset and move the features associated to the original column under the new column name.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      You can also rename a column using [`~DatasetDict.map`] with `remove_columns` but the present method:\n",
      " |          - takes care of moving the original features under the new column name.\n",
      " |          - doesn't copy the data to a new dataset and is thus much faster.\n",
      " |\n",
      " |      Args:\n",
      " |          original_column_name (`str`):\n",
      " |              Name of the column to rename.\n",
      " |          new_column_name (`str`):\n",
      " |              New name for the column.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds = ds.rename_column(\"label\", \"label_new\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label_new'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  rename_columns(self, column_mapping: dict[str, str]) -> 'DatasetDict'\n",
      " |      Rename several columns in the dataset, and move the features associated to the original columns under\n",
      " |      the new column names.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          column_mapping (`Dict[str, str]`):\n",
      " |              A mapping of columns to rename to their new names.\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]: A copy of the dataset with renamed columns.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text_new', 'label_new'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text_new', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text_new', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  reset_format(self)\n",
      " |      Reset `__getitem__` return format to python objects and all columns.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      Same as `self.set_format()`\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      >>> ds.reset_format()\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      ```\n",
      " |\n",
      " |  save_to_disk(\n",
      " |      self,\n",
      " |      dataset_dict_path: Union[str, bytes, os.PathLike],\n",
      " |      max_shard_size: Union[str, int, NoneType] = None,\n",
      " |      num_shards: Optional[dict[str, int]] = None,\n",
      " |      num_proc: Optional[int] = None,\n",
      " |      storage_options: Optional[dict] = None\n",
      " |  )\n",
      " |      Saves a dataset dict to a filesystem using `fsspec.spec.AbstractFileSystem`.\n",
      " |\n",
      " |      For [`Image`], [`Audio`] and [`Video`] data:\n",
      " |\n",
      " |      All the Image(), Audio() and Video() data are stored in the arrow files.\n",
      " |      If you want to store paths or urls, please use the Value(\"string\") type.\n",
      " |\n",
      " |      Args:\n",
      " |          dataset_dict_path (`path-like`):\n",
      " |              Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n",
      " |              of the dataset dict directory where the dataset dict will be saved to.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      " |              The maximum size of the dataset shards to be saved to the filesystem. If expressed as a string, needs to be digits followed by a unit\n",
      " |              (like `\"50MB\"`).\n",
      " |          num_shards (`Dict[str, int]`, *optional*):\n",
      " |              Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.\n",
      " |              You need to provide the number of shards for each dataset in the dataset dictionary.\n",
      " |              Use a dictionary to define a different num_shards for each split.\n",
      " |\n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          num_proc (`int`, *optional*, default `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              Multiprocessing is disabled by default.\n",
      " |\n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |\n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      >>> dataset_dict.save_to_disk(\"path/to/dataset/directory\")\n",
      " |      >>> dataset_dict.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n",
      " |      >>> dataset_dict.save_to_disk(\"path/to/dataset/directory\", num_shards={\"train\": 1024, \"test\": 8})\n",
      " |      ```\n",
      " |\n",
      " |  select_columns(self, column_names: Union[str, list[str]]) -> 'DatasetDict'\n",
      " |      Select one or several column(s) from each split in the dataset and\n",
      " |      the features associated to the column(s).\n",
      " |\n",
      " |      The transformation is applied to all the splits of the dataset\n",
      " |      dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          column_names (`Union[str, list[str]]`):\n",
      " |              Name of the column(s) to keep.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.select_columns(\"text\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  set_format(\n",
      " |      self,\n",
      " |      type: Optional[str] = None,\n",
      " |      columns: Optional[list] = None,\n",
      " |      output_all_columns: bool = False,\n",
      " |      **format_kwargs\n",
      " |  )\n",
      " |      Set `__getitem__` return format (type and columns).\n",
      " |      The format is set for every dataset in the dataset dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          type (`str`, *optional*):\n",
      " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'jax', 'arrow', 'pandas', 'polars']`.\n",
      " |              `None` means `__getitem__` returns python objects (default).\n",
      " |          columns (`list[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              `None` means `__getitem__` returns all columns (default).\n",
      " |          output_all_columns (`bool`, defaults to False):\n",
      " |              Keep un-formatted columns as well in the output (as python objects),\n",
      " |          **format_kwargs (additional keyword arguments):\n",
      " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |\n",
      " |      It is possible to call `map` after calling `set_format`. Since `map` may add new columns, then the list of formatted columns\n",
      " |      gets updated. In this case, if you apply `map` on a dataset to add a new column, then this column will be formatted:\n",
      " |\n",
      " |          `new formatted columns = (all columns - previously unformatted columns)`\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      ```\n",
      " |\n",
      " |  set_transform(\n",
      " |      self,\n",
      " |      transform: Optional[Callable],\n",
      " |      columns: Optional[list] = None,\n",
      " |      output_all_columns: bool = False\n",
      " |  )\n",
      " |      Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.\n",
      " |      The transform is set for every dataset in the dataset dictionary\n",
      " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n",
      " |\n",
      " |      Args:\n",
      " |          transform (`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
      " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in ``__getitem__``.\n",
      " |          columns (`list[str]`, optional): columns to format in the output\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      " |\n",
      " |  shuffle(\n",
      " |      self,\n",
      " |      seeds: Union[int, dict[str, Optional[int]], NoneType] = None,\n",
      " |      seed: Optional[int] = None,\n",
      " |      generators: Optional[dict[str, numpy.random._generator.Generator]] = None,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      load_from_cache_file: Optional[bool] = None,\n",
      " |      indices_cache_file_names: Optional[dict[str, Optional[str]]] = None,\n",
      " |      writer_batch_size: Optional[int] = 1000\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |\n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |\n",
      " |      Args:\n",
      " |          seeds (`Dict[str, int]` or `int`, *optional*):\n",
      " |              A seed to initialize the default BitGenerator if `generator=None`.\n",
      " |              If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |              You can provide one `seed` per dataset in the dataset dictionary.\n",
      " |          seed (`int`, *optional*):\n",
      " |              A seed to initialize the default BitGenerator if `generator=None`. Alias for seeds (a `ValueError` is raised if both are provided).\n",
      " |          generators (`Dict[str, *optional*, np.random.Generator]`):\n",
      " |              Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n",
      " |              You have to provide one `generator` per dataset in the dataset dictionary.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_names (`Dict[str, str]`, *optional*):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices mappings instead of the automatically generated cache file name.\n",
      " |              You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"][\"label\"][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |\n",
      " |      # set a seed\n",
      " |      >>> shuffled_ds = ds.shuffle(seed=42)\n",
      " |      >>> shuffled_ds[\"train\"][\"label\"][:10]\n",
      " |      [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      " |      ```\n",
      " |\n",
      " |  sort(\n",
      " |      self,\n",
      " |      column_names: Union[str, collections.abc.Sequence[str]],\n",
      " |      reverse: Union[bool, collections.abc.Sequence[bool]] = False,\n",
      " |      null_placement: str = 'at_end',\n",
      " |      keep_in_memory: bool = False,\n",
      " |      load_from_cache_file: Optional[bool] = None,\n",
      " |      indices_cache_file_names: Optional[dict[str, Optional[str]]] = None,\n",
      " |      writer_batch_size: Optional[int] = 1000\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create a new dataset sorted according to a single or multiple columns.\n",
      " |\n",
      " |      Args:\n",
      " |          column_names (`Union[str, Sequence[str]]`):\n",
      " |              Column name(s) to sort by.\n",
      " |          reverse (`Union[bool, Sequence[bool]]`, defaults to `False`):\n",
      " |              If `True`, sort by descending order rather than ascending. If a single bool is provided,\n",
      " |              the value is applied to the sorting of all column names. Otherwise a list of bools with the\n",
      " |              same length and order as column_names must be provided.\n",
      " |          null_placement (`str`, defaults to `at_end`):\n",
      " |              Put `None` values at the beginning if `at_start` or `first` or at the end if `at_end` or `last`\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Keep the sorted indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
      " |              If a cache file storing the sorted indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n",
      " |              Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices mapping instead of the automatically generated cache file name.\n",
      " |              You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (`int`, defaults to `1000`):\n",
      " |              Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes')\n",
      " |      >>> ds['train']['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      >>> sorted_ds = ds.sort('label')\n",
      " |      >>> sorted_ds['train']['label'][:10]\n",
      " |      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " |      >>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n",
      " |      >>> another_sorted_ds['train']['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      ```\n",
      " |\n",
      " |  unique(self, column: str) -> dict[str, list]\n",
      " |      Return a list of the unique elements in a column for each split.\n",
      " |\n",
      " |      This is implemented in the low-level backend and as such, very fast.\n",
      " |\n",
      " |      Args:\n",
      " |          column (`str`):\n",
      " |              column name (list all the column names with [`~datasets.DatasetDict.column_names`])\n",
      " |\n",
      " |      Returns:\n",
      " |          Dict[`str`, `list`]: Dictionary of unique elements in the given column.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.unique(\"label\")\n",
      " |      {'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}\n",
      " |      ```\n",
      " |\n",
      " |  with_format(\n",
      " |      self,\n",
      " |      type: Optional[str] = None,\n",
      " |      columns: Optional[list] = None,\n",
      " |      output_all_columns: bool = False,\n",
      " |      **format_kwargs\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
      " |      The format `type` (for example \"numpy\") is used to format batches when using `__getitem__`.\n",
      " |      The format is set for every dataset in the dataset dictionary.\n",
      " |\n",
      " |      It's also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].\n",
      " |\n",
      " |      Contrary to [`~datasets.DatasetDict.set_format`], `with_format` returns a new [`DatasetDict`] object with new [`Dataset`] objects.\n",
      " |\n",
      " |      Args:\n",
      " |          type (`str`, *optional*):\n",
      " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'jax', 'arrow', 'pandas', 'polars']`.\n",
      " |              `None` means `__getitem__` returns python objects (default).\n",
      " |          columns (`list[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              `None` means `__getitem__` returns all columns (default).\n",
      " |          output_all_columns (`bool`, defaults to `False`):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |          **format_kwargs (additional keyword arguments):\n",
      " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      >>> ds = ds.with_format(\"torch\")\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'torch'}\n",
      " |      >>> ds[\"train\"][0]\n",
      " |      {'text': 'compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n",
      " |       'label': tensor(1),\n",
      " |       'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n",
      " |              1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n",
      " |              1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102,     0,\n",
      " |                  0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      " |                  0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      " |                  0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      " |                  0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      " |                  0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      " |                  0,     0,     0,     0]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      " |       'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |              1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      " |      ```\n",
      " |\n",
      " |  with_transform(\n",
      " |      self,\n",
      " |      transform: Optional[Callable],\n",
      " |      columns: Optional[list] = None,\n",
      " |      output_all_columns: bool = False\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n",
      " |      The transform is set for every dataset in the dataset dictionary\n",
      " |\n",
      " |      As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n",
      " |\n",
      " |      Contrary to [`~datasets.DatasetDict.set_transform`], `with_transform` returns a new [`DatasetDict`] object with new [`Dataset`] objects.\n",
      " |\n",
      " |      Args:\n",
      " |          transform (`Callable`, *optional*):\n",
      " |              User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n",
      " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in `__getitem__`.\n",
      " |          columns (`list[str]`, *optional*):\n",
      " |              Columns to format in the output.\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (`bool`, defaults to False):\n",
      " |              Keep un-formatted columns as well in the output (as python objects).\n",
      " |              If set to `True`, then the other un-formatted columns are kept with the output of the transform.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> def encode(example):\n",
      " |      ...     return tokenizer(example['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
      " |      >>> ds = ds.with_transform(encode)\n",
      " |      >>> ds[\"train\"][0]\n",
      " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
      " |       'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,\n",
      " |              112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,\n",
      " |              112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,\n",
      " |              170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,\n",
      " |              179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,\n",
      " |              188,  1566,  7912, 14516,  6997,   119,   102]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  from_csv(\n",
      " |      path_or_paths: dict[str, typing.Union[str, bytes, os.PathLike]],\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      cache_dir: str = None,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      **kwargs\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create [`DatasetDict`] from CSV file(s).\n",
      " |\n",
      " |      Args:\n",
      " |          path_or_paths (`dict` of path-like):\n",
      " |              Path(s) of the CSV file(s).\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (str, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`pandas.read_csv`].\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})\n",
      " |      ```\n",
      " |\n",
      " |  from_json(\n",
      " |      path_or_paths: dict[str, typing.Union[str, bytes, os.PathLike]],\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      cache_dir: str = None,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      **kwargs\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create [`DatasetDict`] from JSON Lines file(s).\n",
      " |\n",
      " |      Args:\n",
      " |          path_or_paths (`path-like` or list of `path-like`):\n",
      " |              Path(s) of the JSON Lines file(s).\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (str, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`JsonConfig`].\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})\n",
      " |      ```\n",
      " |\n",
      " |  from_parquet(\n",
      " |      path_or_paths: dict[str, typing.Union[str, bytes, os.PathLike]],\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      cache_dir: str = None,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      columns: Optional[list[str]] = None,\n",
      " |      **kwargs\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create [`DatasetDict`] from Parquet file(s).\n",
      " |\n",
      " |      Args:\n",
      " |          path_or_paths (`dict` of path-like):\n",
      " |              Path(s) of the CSV file(s).\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          columns (`list[str]`, *optional*):\n",
      " |              If not `None`, only these columns will be read from the file.\n",
      " |              A column name may be a prefix of a nested field, e.g. 'a' will select\n",
      " |              'a.b', 'a.c', and 'a.d.e'.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`ParquetConfig`].\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})\n",
      " |      ```\n",
      " |\n",
      " |  from_text(\n",
      " |      path_or_paths: dict[str, typing.Union[str, bytes, os.PathLike]],\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      cache_dir: str = None,\n",
      " |      keep_in_memory: bool = False,\n",
      " |      **kwargs\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Create [`DatasetDict`] from text file(s).\n",
      " |\n",
      " |      Args:\n",
      " |          path_or_paths (`dict` of path-like):\n",
      " |              Path(s) of the text file(s).\n",
      " |          features ([`Features`], *optional*):\n",
      " |              Dataset features.\n",
      " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
      " |              Directory to cache data.\n",
      " |          keep_in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Keyword arguments to be passed to [`TextConfig`].\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})\n",
      " |      ```\n",
      " |\n",
      " |  load_from_disk(\n",
      " |      dataset_dict_path: Union[str, bytes, os.PathLike],\n",
      " |      keep_in_memory: Optional[bool] = None,\n",
      " |      storage_options: Optional[dict] = None\n",
      " |  ) -> 'DatasetDict'\n",
      " |      Load a dataset that was previously saved using [`save_to_disk`] from a filesystem using `fsspec.spec.AbstractFileSystem`.\n",
      " |\n",
      " |      Args:\n",
      " |          dataset_dict_path (`path-like`):\n",
      " |              Path (e.g. `\"dataset/train\"`) or remote URI (e.g. `\"s3//my-bucket/dataset/train\"`)\n",
      " |              of the dataset dict directory where the dataset dict will be loaded from.\n",
      " |          keep_in_memory (`bool`, defaults to `None`):\n",
      " |              Whether to copy the dataset in-memory. If `None`, the\n",
      " |              dataset will not be copied in-memory unless explicitly enabled by setting\n",
      " |              `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n",
      " |              [improve performance](../cache#improve-performance) section.\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
      " |\n",
      " |              <Added version=\"2.8.0\"/>\n",
      " |\n",
      " |      Returns:\n",
      " |          [`DatasetDict`]\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> ds = load_from_disk('path/to/dataset/directory')\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  cache_files\n",
      " |      The cache files containing the Apache Arrow table backing each split.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.cache_files\n",
      " |      {'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],\n",
      " |       'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],\n",
      " |       'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}\n",
      " |      ```\n",
      " |\n",
      " |  column_names\n",
      " |      Names of the columns in each split of the dataset.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.column_names\n",
      " |      {'test': ['text', 'label'],\n",
      " |       'train': ['text', 'label'],\n",
      " |       'validation': ['text', 'label']}\n",
      " |      ```\n",
      " |\n",
      " |  data\n",
      " |      The Apache Arrow tables backing each split.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.data\n",
      " |      ```\n",
      " |\n",
      " |  num_columns\n",
      " |      Number of columns in each split of the dataset.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.num_columns\n",
      " |      {'test': 2, 'train': 2, 'validation': 2}\n",
      " |      ```\n",
      " |\n",
      " |  num_rows\n",
      " |      Number of rows in each split of the dataset.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.num_rows\n",
      " |      {'test': 1066, 'train': 8530, 'validation': 1066}\n",
      " |      ```\n",
      " |\n",
      " |  shape\n",
      " |      Shape of each split of the dataset (number of rows, number of columns).\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> ds.shape\n",
      " |      {'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __orig_bases__ = (dict[typing.Union[str, datasets.splits.NamedSplit], ...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |\n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |\n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |\n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |\n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |\n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |\n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __ior__(self, value, /)\n",
      " |      Return self|=value.\n",
      " |\n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |\n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |\n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |\n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |\n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |\n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |\n",
      " |  __reversed__(self, /)\n",
      " |      Return a reverse iterator over the dict keys.\n",
      " |\n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |\n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |\n",
      " |  __sizeof__(self, /)\n",
      " |      Return the size of the dict in memory, in bytes.\n",
      " |\n",
      " |  clear(self, /)\n",
      " |      Remove all items from the dict.\n",
      " |\n",
      " |  copy(self, /)\n",
      " |      Return a shallow copy of the dict.\n",
      " |\n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |\n",
      " |  items(self, /)\n",
      " |      Return a set-like object providing a view on the dict's items.\n",
      " |\n",
      " |  keys(self, /)\n",
      " |      Return a set-like object providing a view on the dict's keys.\n",
      " |\n",
      " |  pop(self, key, default=<unrepresentable>, /)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |\n",
      " |      If the key is not found, return the default if given; otherwise,\n",
      " |      raise a KeyError.\n",
      " |\n",
      " |  popitem(self, /)\n",
      " |      Remove and return a (key, value) pair as a 2-tuple.\n",
      " |\n",
      " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      " |      Raises KeyError if the dict is empty.\n",
      " |\n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |\n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E.keys(): D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |\n",
      " |  values(self, /)\n",
      " |      Return an object providing a view on the dict's values.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |\n",
      " |  __class_getitem__(object, /)\n",
      " |      See PEP 585\n",
      " |\n",
      " |  fromkeys(iterable, value=None, /)\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |\n",
      " |  __new__(*args, **kwargs) class method of builtins.dict\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |\n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e4592e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'inputs', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'multi_label', 'explanation', 'id', 'metadata', 'status', 'event_timestamp', 'metrics', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 20491\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    do_train=False,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_delay=0,\n",
    "    eval_do_concat_batches=True,\n",
    "    eval_on_start=False,\n",
    "    eval_steps=None,\n",
    "    eval_strategy='epoch',\n",
    "    eval_use_gather_object=False,\n",
    "    evaluation_strategy='epoch',\n",
    "    fp16=False,\n",
    "    fp16_backend='auto',\n",
    "    fp16_full_eval=False,\n",
    "    fp16_opt_level='O1',\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs=None,\n",
    "    hub_strategy='every_save',\n",
    "    learning_rate=5e-05,\n",
    "    logging_dir='output_dir/runs/Jan19_07-20-19_0b6b572e-c52f-4580-96c7-1f579483693e',\n",
    "    logging_steps=500,\n",
    "    logging_strategy='steps',\n",
    "    lr_scheduler_kwargs={},\n",
    "    lr_scheduler_type='linear',\n",
    "    max_grad_norm=1.0,\n",
    "    max_steps=-1,\n",
    "    neftune_noise_alpha=None,\n",
    "    no_cuda=False,\n",
    "    num_train_epochs=3.0,\n",
    "    optim='adamw_torch',\n",
    "    optim_args=None,\n",
    "    optim_target_modules=None,\n",
    "    output_dir='output_dir',\n",
    "    overwrite_output_dir=False,\n",
    "    past_index=-1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    prediction_loss_only=False,\n",
    "    push_to_hub=False,\n",
    "    push_to_hub_model_id=None,\n",
    "    push_to_hub_organization=None,\n",
    "    save_on_each_node=False,\n",
    "    save_only_model=False,\n",
    "    save_safetensors=True,\n",
    "    save_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    "    skip_memory_metrics=True,\n",
    "    split_batches=None,\n",
    "    tf32=None,\n",
    "    warmup_ratio=0.0,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.0,\n",
    ")\n",
    "\n",
    "# Define the train and test datasets\n",
    "training_dataset = tokenized_datasets['train']\n",
    "testing_dataset = tokenized_datasets['test']\n",
    "\n",
    "# Initialize the trainer class\n",
    "trainer = Trainer(\n",
    "# Add arguments to the class\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=testing_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026a759",
   "metadata": {},
   "source": [
    "### Loading the Prompt and Preference Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd66920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column(['Hi, I want to learn to play horseshoes. Can you teach me?'])\n"
     ]
    }
   ],
   "source": [
    "preference_dataset = load_dataset('trl-internal-testing/hh-rlhf-helpful-base-trl-style',split='train')\n",
    "\n",
    "def extract_prompt(text):\n",
    "    return text[0]['content']\n",
    "\n",
    "preference_dataset_with_prompt = preference_dataset.map(lambda sample: {**sample, 'prompt': extract_prompt(sample['chosen'])})\n",
    "\n",
    "sample = preference_dataset_with_prompt.select(range(1))\n",
    "print(sample['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41879c3a",
   "metadata": {},
   "source": [
    "### Detecting Anomalous Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41108a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "prob_dists = np.array([np.array([[0.53463835, 0.46536162]]),\n",
    " np.array([[0.6142045 , 0.38579544]]),\n",
    " np.array([[0.5121281 , 0.48787192]])])\n",
    "\n",
    "texts = ['This movie was fantastic!', \"I don't know if I liked it or not.\", 'The book was incredibly boring.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "175faef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-confidence texts: ['This movie was fantastic!', \"I don't know if I liked it or not.\", 'The book was incredibly boring.']\n"
     ]
    }
   ],
   "source": [
    "def least_confidence(prob_dist):    \n",
    "    simple_least_conf = np.nanmax(prob_dist)\n",
    "    num_labels = float(prob_dist.size)  # number of labels\n",
    "    least_conf = (1 - simple_least_conf) * (num_labels / (num_labels - 1))\n",
    "    return least_conf\n",
    "\n",
    "def filter_low_confidence_predictions(prob_dists, threshold=0.5):\n",
    "    filtered_indices = [i for i, prob_dist in enumerate(prob_dists) if least_confidence(prob_dist) > threshold]\n",
    "    return filtered_indices\n",
    "\n",
    "filtered_indices = filter_low_confidence_predictions(prob_dists)\n",
    "\n",
    "high_confidence_texts = [texts[i] for i in filtered_indices]\n",
    "print(\"High-confidence texts:\", high_confidence_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c5af9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.02 0.   0.02]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster._kmeans import KMeans\n",
    "\n",
    "def detect_anomalies(data, n_clusters=3):\n",
    "    # Initialize k-means\n",
    "    kmeans=KMeans(n_clusters,random_state=42)\n",
    "    clusters = kmeans.fit_predict(data)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Calculate distances from cluster centers\n",
    "    distances = np.linalg.norm(data-centers[clusters],axis=1)\n",
    "    return distances\n",
    "  \n",
    "confidences = np.array([[0.34],\n",
    "       [0.72],\n",
    "       [0.51],\n",
    "       [0.68]])\n",
    "\n",
    "anomalies = detect_anomalies(confidences)\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e098b1",
   "metadata": {},
   "source": [
    "### Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9fa0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/modAL-python/modAL.git\n",
      "  Cloning https://github.com/modAL-python/modAL.git to c:\\users\\cheku\\appdata\\local\\temp\\pip-req-build-qltecfs0\n",
      "  Resolved https://github.com/modAL-python/modAL.git to commit bba6f6fd00dbb862b1e09259b78caf6cffa2e755\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from modAL-python==0.4.2) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from modAL-python==0.4.2) (1.8.0)\n",
      "Requirement already satisfied: scipy>=0.18 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from modAL-python==0.4.2) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from modAL-python==0.4.2) (2.3.3)\n",
      "Collecting skorch==0.9.0 (from modAL-python==0.4.2)\n",
      "  Downloading skorch-0.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tabulate>=0.7.7 (from skorch==0.9.0->modAL-python==0.4.2)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from skorch==0.9.0->modAL-python==0.4.2) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from pandas>=1.1.0->modAL-python==0.4.2) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.0->modAL-python==0.4.2) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from scikit-learn>=0.18->modAL-python==0.4.2) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from scikit-learn>=0.18->modAL-python==0.4.2) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cheku\\genai-roadmap\\env\\lib\\site-packages (from tqdm>=4.14.0->skorch==0.9.0->modAL-python==0.4.2) (0.4.6)\n",
      "Downloading skorch-0.9.0-py3-none-any.whl (125 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Building wheels for collected packages: modAL-python\n",
      "  Building wheel for modAL-python (pyproject.toml): started\n",
      "  Building wheel for modAL-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for modAL-python: filename=modal_python-0.4.2-py3-none-any.whl size=32999 sha256=dd4f7e0e9f86826b6d3cb977e27fc0a23f62b22561579264739dfbaea5b4ec98\n",
      "  Stored in directory: C:\\Users\\cheku\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-acq7j_wa\\wheels\\73\\44\\60\\526028e94d205df2a0991c99476d1298559184803e4d6d774a\n",
      "Successfully built modAL-python\n",
      "Installing collected packages: tabulate, skorch, modAL-python\n",
      "\n",
      "   ------------- -------------------------- 1/3 [skorch]\n",
      "   ---------------------------------------- 3/3 [modAL-python]\n",
      "\n",
      "Successfully installed modAL-python-0.4.2 skorch-0.9.0 tabulate-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/modAL-python/modAL.git 'C:\\Users\\cheku\\AppData\\Local\\Temp\\pip-req-build-qltecfs0'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/modAL-python/modAL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c365ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models.learners import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_labeled = np.array([[ 0.57985729,  1.14796125,  1.21897079, ..., -0.12951725,\n",
    "         1.00561139, -0.05658849],\n",
    "       [-0.21048803, -0.83714743, -0.46264474, ..., -0.98991531,\n",
    "        -0.67658159,  1.23661974],\n",
    "       [ 0.86018521,  0.48524158,  0.96373788, ...,  1.39659785,\n",
    "         0.36755707,  1.11643075],\n",
    "       ...,\n",
    "       [-1.44311529, -1.39557951, -0.87960631, ..., -2.23979163,\n",
    "        -1.20240545, -0.99144228],\n",
    "       [ 0.94686146, -1.73188868,  0.91339109, ...,  0.06429488,\n",
    "        -1.46065958,  1.35108895],\n",
    "       [ 1.18705115,  0.83332305, -0.28231204, ...,  0.31912084,\n",
    "         0.64395896, -0.79062399]])\n",
    "\n",
    "y_labeled=np.array([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
    "       0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
    "       1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
    "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
    "       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1])\n",
    "\n",
    "# Create the active learner object\n",
    "learner = ActiveLearner(\n",
    "    # Set the estimator \n",
    "    estimator=LogisticRegression(),\n",
    "    # Set the query strategy\n",
    "    query_strategy = uncertainty_sampling,\n",
    "    # Pass the labeled data\n",
    "    X_training=X_labeled, y_training=y_labeled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries=10\n",
    "for _ in range(n_queries):\n",
    "    # Use the current labeled data\n",
    "    learner.teach(X_labeled,y_labeled)\n",
    "    # Query from unlabeled data\n",
    "    query_idx, _ = learner.query(X_unlabeled,n_instances=5)\n",
    "    X_new, y_new = X_unlabeled[query_idx], y[query_idx]  \n",
    "    X_labeled = np.vstack((X_labeled, X_new))  \n",
    "    y_labeled = np.append(y_labeled, y_new)  \n",
    "    # Update the unlabeled dataset\n",
    "    X_unlabeled = np.delete(X_unlabeled, query_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232c8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
